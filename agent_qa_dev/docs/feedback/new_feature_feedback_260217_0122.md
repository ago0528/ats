# LLMOps 백오피스 기능 스펙

> 에이전트 품질 검증을 위한 쿼리 관리 및 검증 시스템  
> 작성일: 2025년 2월 | 작성자: Midas Group

---

## 1. 문서 개요

### 목적

서비스에서 운영 중인 AI 에이전트의 응답 품질을 체계적으로 테스트하고 평가하기 위한 백오피스를 구축합니다. 서비스에서 사용 중인 에이전트와 쿼리 API를 그대로 활용하여, 별도로 붙인 백오피스 형태로 운영합니다.

### 메뉴 구성

- **쿼리 관리**: 테스트용 질문을 사전 등록하고 분류/관리하는 메뉴
- **검증 관리**: 등록된 쿼리 또는 직접 입력한 질의로 에이전트를 테스트하고, 평가 결과를 확인하는 메뉴

### 설계 전제

- 에이전트 목록은 서비스에서 운영 중인 것을 하드코딩으로 관리 (별도 CRUD 없음)
- 쿼리 API는 서비스에서 사용 중인 것을 그대로 활용
- LLM 평가 지표는 쿼리/그룹별로 커스터마이징 가능 (에이전트 플로우마다 가점/감점 기준이 다름)
- 평가 모델과 테스트 대상 모델을 별도로 선택 가능

---

## 2. 쿼리 관리

에이전트 검증용 질문을 사전에 등록하고, 분류하고, 관리하는 메뉴입니다.

### 2.1 쿼리 데이터 모델

| 필드명 | 타입 | 필수 여부 | 설명 |
|---|---|---|---|
| id | UUID | 자동 생성 | Primary Key |
| query_text | TEXT | 필수 | 실제 테스트할 질문 텍스트 |
| expected_result | TEXT | 필수 | 이 질문에 대한 정답/기대 응답 |
| category | ENUM | 필수 | Happy path / Edge case / Adversarial input |
| group_id | FK | 필수 | 소속 그룹 (에이전트 플로우 단위) |
| llm_eval_criteria | JSON | 선택 | 평가 지표별 기준 텍스트 (쿼리 레벨 커스터마이징) |
| created_by | VARCHAR | 자동 기록 | 등록자 |
| created_at | TIMESTAMP | 자동 | 생성일시 |
| updated_at | TIMESTAMP | 자동 | 수정일시 |

### 2.2 쿼리 카테고리

| 카테고리 | 설명 | 예시 |
|---|---|---|
| Happy path | 정상적인 사용 시나리오 | "서울 강남 개발자 채용 공고 보여줘" |
| Edge case | 경계값 및 예외 상황 | ""입력 없이 엔터"" / 특수문자 포함 질의 |
| Adversarial input | 악의적 입력 및 탈옥 시도 | 프롬프트 인젝션 / 역할 변경 시도 |

### 2.3 그룹 관리

그룹은 에이전트 플로우 단위로 쿼리를 묶는 단위입니다. 검증 결과를 그룹별로 모아 볼 수 있으며, 별도 CRUD가 가능합니다.

| 필드명 | 타입 | 설명 |
|---|---|---|
| id | UUID | Primary Key |
| group_name | VARCHAR | 그룹명 (예: 이동 에이전트, 지원자 관리 에이전트) |
| llm_eval_criteria_default | JSON | 그룹 레벨 기본 평가 지표 (쿼리에 별도 설정 없으면 이 기준 적용) |
| description | TEXT | 그룹 설명 |
| created_at | TIMESTAMP | 생성일시 |

> **평가 지표 적용 우선순위**: 쿼리 레벨 `llm_eval_criteria` > 그룹 레벨 `llm_eval_criteria_default`  
> 쿼리에 별도 지표가 설정되어 있으면 그것을 우선 적용하고, 없으면 그룹의 기본값을 사용합니다.

### 2.4 주요 기능

- **CRUD**: 쿼리 등록, 수정, 삭제, 목록 조회
- **필터링/검색**: 카테고리별, 그룹별, 키워드 검색
- **그룹 관리**: 그룹 생성/수정/삭제 (서브 메뉴 또는 모달)
- **벌크 등록**: CSV/엑셀로 다수의 쿼리를 한 번에 업로드
- **검증 이력 연결**: 각 쿼리에서 해당 쿼리로 실행된 검증 이력 바로 확인 가능

---

## 3. 검증 관리

등록된 쿼리 또는 직접 입력한 질의로 에이전트를 테스트하고, 평가 결과를 확인하는 메뉴입니다.

### 3.1 검증 실행 모드

#### Mode A: 등록 쿼리 기반 검증

- 쿼리 목록에서 단건 또는 복수 선택 후 실행
- 그룹 단위 일괄 실행 지원 (예: "이동 에이전트 그룹 전체 테스트")
- 선택 후 대상 에이전트 및 모델 지정 → 실행
- 기등록된 기대결과와 평가기준이 자동 적용됨

#### Mode B: 직접 입력 검증

- 자유 텍스트로 질의 입력
- 기대결과, LLM 평가기준은 선택 입력 (입력 시 평가에 활용, 미입력 시 LLM 자체 판단)
- 일회성 테스트 용도
- 검증 후 쿼리로 저장 가능 (**수동 저장** 방식)

### 3.2 검증 실행 공통 설정

| 설정 항목 | 타입 | 설명 |
|---|---|---|
| 대상 에이전트 | 선택 (하드코딩 목록) | 서비스에서 운영 중인 에이전트 중 선택 |
| 테스트 대상 모델 | 선택 | 에이전트가 사용할 LLM (GPT-4o, Claude Sonnet 등) |
| 평가 모델 | 선택 | LLM 평가에 사용할 모델 (테스트 대상과 다를 수 있음) |
| 반복 횟수 | NUMBER | 동일 쿼리 N회 반복 (일관성 검증용) |
| 타임아웃 | NUMBER (ms) | 응답 제한 시간 |

### 3.3 Response 데이터 모델

| 필드명 | 타입 | 설명 |
|---|---|---|
| id | UUID | Primary Key |
| query_id | FK (nullable) | 연결된 쿼리 (Mode B 직접 입력 시 null) |
| conversation_id | VARCHAR | query API Response에 포함된 질의의 Conversation ID 값(데이터독 로그 확인 시 활용하기 위함) |
| agent_id | VARCHAR | 테스트한 에이전트 식별자 |
| raw_response | TEXT | 에이전트가 반환한 원본 응답 |
| test_model | VARCHAR | 실행 시 사용된 모델 |
| eval_model | VARCHAR | LLM 평가에 사용된 모델 |
| latency_ms | INTEGER | 응답 시간 (ms) |
| executed_at | TIMESTAMP | 실행 일시 |

### 3.4 평가 체계

각 response에 대해 두 가지 평가가 병렬로 실행됩니다.

#### Logic Evaluation (로직 평가)

규칙 기반으로 응답의 정합성을 검증합니다.

| 필드명 | 타입 | 설명 |
|---|---|---|
| id | UUID | Primary Key |
| query_id | FK | 연결된 쿼리 |
| response_id | FK | 연결된 응답 |
| eval_items | JSON | 평가 항목별 Pass/Fail (필수 키워드, URL 유효성, 포맷 준수 등) |
| result | ENUM | 종합 Pass / Fail |
| fail_reason | TEXT | 실패 시 불충족 조건 상세 |
| evaluated_at | TIMESTAMP | 평가 일시 |

#### LLM Evaluation (LLM 평가)

LLM을 활용하여 응답 품질을 다차원으로 평가합니다. 평가 지표는 쿼리/그룹별로 커스터마이징됩니다.

| 필드명 | 타입 | 설명 |
|---|---|---|
| id | UUID | Primary Key |
| query_id | FK | 연결된 쿼리 |
| response_id | FK | 연결된 응답 |
| eval_model | VARCHAR | 평가에 사용된 LLM 모델 |
| metric_scores | JSON | 지표별 스코어 (예: {"정확성": 4, "완전성": 5, "톤": 3}) |
| total_score | FLOAT | 종합 스코어 (가중 평균 또는 단순 평균) |
| llm_comment | TEXT | 평가 모델이 남긴 텍스트 피드백 |
| evaluated_at | TIMESTAMP | 평가 일시 |

#### 평가 지표 커스터마이징

에이전트 플로우마다 가점/감점 기준이 다를 수 있으므로, 평가 지표는 두 레벨에서 설정 가능합니다:

- **그룹 레벨**: 해당 그룹에 속한 모든 쿼리에 기본 적용되는 평가 지표 세트
- **쿼리 레벨**: 개별 쿼리에 특화된 평가 지표 (설정 시 그룹 기본값 대신 적용)

평가 지표 예시 (JSON 구조):

| 지표명 | 설명 | 스코어 범위 | 가중치 |
|---|---|---|---|
| 정확성 | 응답 내용이 사실과 일치하는지 | 1~5 | 0.4 |
| 완전성 | 필요한 정보가 빠짐없이 포함되었는지 | 1~5 | 0.3 |
| 톤앤매너 | 응답 어조가 서비스 기준에 맞는지 | 1~5 | 0.15 |
| 안전성 | 유해한 내용이나 할루시네이션 없는지 | 1~5 | 0.15 |

---

## 4. 검증 결과 조회

### 4.1 개별 조회

특정 쿼리에 대한 응답과 Logic/LLM 평가 결과를 한 화면에서 비교 확인합니다.

- 질의 텍스트 + 기대결과
- 에이전트 원본 응답 (raw_response)
- Logic 평가: Pass/Fail + 상세 사유
- LLM 평가: 지표별 스코어 + 종합 스코어 + 피드백 코멘트
- 메타 정보: 사용 모델, 평가 모델, latency, 실행 일시

### 4.2 그룹별 대시보드

그룹 단위로 검증 결과를 요약 확인합니다.

- 그룹 전체 Pass율 (Logic 평가 기준)
- LLM 평가 평균 스코어 (지표별)
- 실패 패턴 요약: 어떤 카테고리/유형에서 실패가 집중되는지

### 4.3 이력 비교

동일 쿼리의 시간대별 결과 추이를 비교합니다. 모델 변경, 프롬프트 수정 전후 품질 변화를 추적하는 데 활용합니다.

- 필터: 쿼리, 기간, 모델, 에이전트
- 표시: 스코어 추이 그래프, Pass율 변화, latency 변화

---

## 5. 단계별 개발 계획

### Phase 1 (MVP) — 핵심 기능 우선 구축

- 쿼리 CRUD (등록, 수정, 삭제, 목록 조회)
- 그룹 CRUD
- Mode B: 단건 직접 입력 검증
- Response 저장 + 기본 LLM 평가 (1개 지표)
- 검증 결과 개별 조회

### Phase 2 — 등록 쿼리 기반 테스트 + 평가 고도화

- Mode A: 등록 쿼리 기반 검증 (단건/복수/그룹 일괄)
- Logic Evaluation 추가
- LLM 평가 지표 커스터마이징 (그룹/쿼리 레벨)
- 평가 모델 별도 선택 기능
- 직접 입력 검증 결과 → 쿼리로 수동 저장

### Phase 3 — 대시보드 + 운영 효율화

- 벌크 업로드 (CSV/엑셀)
- 그룹별 대시보드 (Pass율, 평균 스코어, 실패 패턴)
- 이력 비교 (시간대별 스코어 추이, 모델/프롬프트 변경 전후 비교)
- 반복 실행 및 일관성 검증

---

## 6. 데이터 관계도 (ER)

주요 엔티티 간 관계:

- **group (1) → (N) query**: 하나의 그룹에 여러 쿼리가 소속
- **query (1) → (N) response**: 하나의 쿼리로 여러 번 검증 실행 가능
- **response (1) → (1) logic_evaluation**: 각 응답에 대해 로직 평가 1건
- **response (1) → (1) llm_evaluation**: 각 응답에 대해 LLM 평가 1건

> **평가 지표 적용 우선순위**: `query.llm_eval_criteria` (not null) > `group.llm_eval_criteria_default` > 시스템 기본값