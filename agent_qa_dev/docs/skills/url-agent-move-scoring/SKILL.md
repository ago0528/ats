---
name: url-agent-move-scoring
description: Use when evaluating 이동 에이전트 raw CSV 결과 across repeated runs and generating markdown score reports.
---

# URL Agent Scoring Skill

## 목표
이동 에이전트(raw CSV) 결과를 입력받아 회차별·세트별로 아래 지표를 산출한다.
- 의도 충족
- 일관성
- 정확성
- 응답 속도(단일 도구)
- 안정성

**총 가중치 합산은 산출하지 않는다.**

---

## 적용 범위
- 대상: 이동 에이전트
- 입력: `docs/test-result/url_agent_*.csv` 형태
- 전제: 동일 질문 세트가 최소 2회 실행됨(예: `1/1`, `2/1`)
- 출력: 마크다운 리포트 + 원본 판정 테이블(필요 시)

---

## 1) 입력 데이터 규격
필수/기준 컬럼

- `Run ID`
- `Item ID`
- `Query ID`
- `질의`
- `기대결과`
- `카테고리`
- `방/반복`
- `응답`
- `오류`
- `LLM 상태`
- `LLM 점수`
- `LLM 코멘트`
- `Raw JSON`

`Raw JSON`은 문자열 JSON.

### Raw JSON 핵심 필드
- `assistantMessage`
- `dataUIList[*].uiValue.buttonUrl`
- `responseTimeSec`
- `error`
- `executionProcesses`, `worker`(보조)

---

## 2) 판정 규칙(기본)

### 2-1. 정확성 (이동 에이전트 datakey 우선)

1. `기대결과(F)`에서 dataKey를 우선 추출한다.
   - 패턴: `dataKey=([A-Z0-9_]+)`
2. `Raw JSON`의 `dataUIList`에서 `buttonUrl`의 query 파싱으로 실제 dataKey를 추출한다.
3. 판정
   - **5**: 기대 dataKey가 실제 URL 집합에 존재
   - **3**: 기대 dataKey 미포함이지만 기대 경로가 동일하고 대체 dataKey가 존재(선택 규칙 허용 시)
   - **0**: 기대 dataKey 미포함 또는 URL 미존재

> 운영에서는 5/0으로 이원화해도 됨.

### 2-2. 의도 충족 (LLM 정성)
`assistantMessage` 중심 정성 채점

- 5: 의도 정확 + 응답 흐름 완전 적절
- 4: 의도는 맞지만 표현/범위가 약간 부정확
- 3: 핵심 의도는 맞으나 일부 중요 요소 누락
- 2: 부분적만 반영
- 1: 관련 영역처럼 보이나 실제 의도는 미반영
- 0: 의도 불일치

> LLM 점수가 없을 경우(또는 신뢰성 낮을 경우) 휴리스틱 기준으로 보조 판정 가능.

### 2-3. 안정성
- Raw JSON 파싱이 성공하고(`error` 빈 값) + `오류` 빈 값이면 **5**
- 그 외는 **0**

### 2-4. 응답 속도 (단일 도구 루브릭, 이 스킬의 핵심)

점수화는 `responseTimeSec`로만 수행하고, 점수는 **초를 구간으로 변환한 값**이다.

| 점수 | 기준 |
| --- | --- |
| 5 | <= 5초 |
| 4 | 5초 초과 ~ 8초 |
| 3 | 8초 초과 ~ 10초 |
| 2 | 10초 초과 ~ 15초 |
| 1 | 15초 초과 ~ 20초 |
| 0 | 20초 초과 또는 타임아웃 |

참고: 점수가 낮다고 해서 ‘조회 실패’가 아니라 “구간 임계치 미달”일 수 있음.

### 2-5. 일관성
같은 `Query ID`의 회차 쌍(A/B)을 맞춰서
- 둘 다 성공(5/3/4/…)
- 둘 다 실패(0)
- 그 외 불일치

일치율 = `(둘 다 성공 + 둘 다 실패) / 전체 질의수`

일관성 점수 = `일치율 × 5`

---

## 3) 집계 규칙

- 문항 점수는 항목 평균으로 집계
- 정확성/의도/안정성/속도: 1~5 점수 평균
- 일관성: `Query ID` 매칭 기반 위 규칙 적용

운영 산식(권장)
- `문항수`는 회차별 동일.
- `세트 평균`은 회차별 값을 단순 평균
- `속도`는 초 + 점수 모두 같이 제시
  - 평균 초(sec)
  - 평균 점수
  - 점수 분포(건수)

---

## 4) 산출물 템플릿(마크다운)

```markdown
# 이동 에이전트 스코어링 요약
- 데이터: <파일명>
- 총 항목: <N>
- 실행: <회차>

## 산출 기준
- 정확성: 기대결과(F) dataKey 우선 매칭
- 회차 매칭 키: Query ID
- 안정성: 오류/Raw error 기준
- 속도: 단일 도구(responseTimeSec)

## 지표별 점수

### 1. 의도 충족
- 1회차: x.xx
- 2회차: x.xx
- 세트: x.xx
- 기준: 휴리스틱 또는 LLM 정성 채점 일치

### 2. 일관성
- 둘 다 정답 / 둘 다 실패 / 불일치
- 일치율: xx.xx%
- 점수: x.xx

### 3. 정확성
| 회차 | 성공률 | 성공 | 실패 | 문항 평균 |
| --- | --- | --- | --- | --- |
| 1회차 | xx% | x | y | x.xx |
| 2회차 | xx% | x | y | x.xx |
| 세트 | xx% | x | y | x.xx |

### 4. 응답 속도(단일)
- 회차별 평균 초/점수:
  - 1회차: xx.xx초 / x.xx
  - 2회차: xx.xx초 / x.xx
  - 세트: xx.xx초 / x.xx
- 분포: `5점(...) , 4점(...) , ...`

### 5. 안정성
- 정상(문항 수): xx
- 에러/타임아웃(문항 수): xx
- 점수: x.xx

## 리스크/개선
- URL 추출 실패 패턴(특정 메뉴 path)
- 동일 query 반복 불일치
- 속도 상위 이상치

## 원본 판정 테이블 (appendix)
- 문항별 의도/정확성/속도/안정성/일관성(pass/fail)
```

---

## 5) LLM용 평가 프롬프트(복붙)

```text
너는 QA 평가자다. 아래 규칙으로 이동 에이전트 raw CSV를 평가해.

[규칙]
1) 기대결과(F)에서 dataKey를 추출한다. 없으면 기대 URL path를 보조 참고한다.
2) Raw JSON에서 assistantMessage, dataUIList의 buttonUrl, responseTimeSec, error를 파싱한다.
3) 정확성: 기대 dataKey가 실제 dataKey 집합에 있으면 5, path만 유사하고 다른 dataKey면 3, 없으면 0.
4) 의도 충족: assistantMessage를 0~5 정성 채점한다.
5) 안정성: 오류 미존재면 5, 있으면 0.
6) 속도: responseTimeSec를 단일 루브릭으로 점수화한다.
7) 동일 Query ID 쌍으로 일관성 계산: 둘 다 성공/둘 다 실패 비율*5.
8) 회차별→세트별로 집계한 뒤 총합 가중치는 계산하지 않는다.

[출력]
- 지표별 회차 점수와 세트 점수
- 속도: 평균 초(sec) + 평균 점수 + 분포
- 일관성(둘 다 정답/둘 다 실패/불일치)
- 안정성 실패 상위 원인
```

---

## 6) 운영 체크리스트
- [ ] 회차별 문항 수가 동일한가
- [ ] Query ID 매칭이 깨지지 않았는가
- [ ] Raw JSON 파싱 실패율이 비정상적으로 높지 않은가
- [ ] 데이터셋 내 LLM 정성 점수 누락 시 휴리스틱 규칙으로 대체했는가
- [ ] Track별/도메인별 실패 패턴 Top-N을 추적했는가
